{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/yuki_uchida/items/09fda4c5c608a9f53d2f#ツイートを文章ベクトルに変換する\n",
    "\n",
    "https://nlp.ist.i.kyoto-u.ac.jp/index.php?ku_bert_japanese　BERT学習済みモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/hoteison/Desktop/DoMyResearch/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import numpy as np\n",
    "model = BertModel.from_pretrained('/Users/hoteison/Desktop/DoMyResearch/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers')\n",
    "bert_tokenizer = BertTokenizer(\"/Users/hoteison/Desktop/DoMyResearch/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt\",\n",
    "                               do_lower_case=False, do_basic_tokenize=False)\n",
    "# 相対パスだとダメだったが、絶対パスなら行けた\n",
    "# もっと重いファイルを選べば精度が上がるらしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://yag-ays.github.io/project/pytorch_bert_japanese/\n",
    "\n",
    "from pyknp import Juman\n",
    "\n",
    "class JumanTokenizer():\n",
    "    def __init__(self):\n",
    "        self.juman = Juman()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        result = self.juman.analysis(text)\n",
    "        return [mrph.midasi for mrph in result.mrph_list()]\n",
    "\n",
    "juman_tokenizer = JumanTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vector(text, model, bert_tokenizer, juman_tokenizer):\n",
    "    use_model = model\n",
    "    tokens = juman_tokenizer.tokenize(text)\n",
    "    bert_tokens = bert_tokenizer.tokenize(\" \".join(tokens))\n",
    "    ids = bert_tokenizer.convert_tokens_to_ids(\n",
    "        [\"[CLS]\"] + bert_tokens[:126] + [\"[SEP]\"])\n",
    "    tokens_tensor = torch.tensor(ids).reshape(1, -1)\n",
    "    use_model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_encoder_layers, _ = use_model(tokens_tensor)\n",
    "    pooling_layer = -2\n",
    "    print(all_encoder_layers[0])\n",
    "    embedding = all_encoder_layers[0][pooling_layer].numpy()\n",
    "    # embedding = all_encoder_layers[0].numpy()\n",
    "    # return np.mean(embedding, axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv(\"/Users/hoteison/Desktop/DoMyResearch/arranged_tripadvisor.csv\", encoding='utf-8')\n",
    "reviews_df[\"body\"] = reviews_df[\"body\"].astype(str) #一応文字列にしておく\n",
    "reviews_df = reviews_df[\"body\"]\n",
    "## 文章ベクトル変換後にその結果を格納するための配列と、元のツイートを保存しておくための配列を宣言する\n",
    "vectors = []\n",
    "reviews = []\n",
    "for review in reviews_df:\n",
    "    review = re.sub('\\n', \" \", review)  #改行文字のの削除\n",
    "    strip_review = re.sub(r'[︰-＠]', \"\", review)  #全角記号の削除\n",
    "    # print(strip_review, len(strip_review))\n",
    "\n",
    "    try:\n",
    "        if len(strip_review) > 3: #単語数が少なすぎると適切なベクトルが得られない可能性があるため\n",
    "            vector = compute_vector(\n",
    "                strip_review, model, bert_tokenizer, juman_tokenizer)\n",
    "            vectors.append(vector)\n",
    "            reviews.append(review)\n",
    "\n",
    "            print(1)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "## 文章ベクトルに変換したものをtsvにおく。(可視化ツールがtsvを要求してくるのでtsvにする)\n",
    "pd.DataFrame(reviews).to_csv('./reviews_text.tsv', index=False, header=None)\n",
    "pd.DataFrame(vectors).to_csv('./reviews_vector.tsv', sep='\\t', index=False, header=None)\n",
    "\n",
    "\"\"\"\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "l\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1212　エラーの理由はよくわかってない\n",
    "・可能性としては、Tweetはテキストの最大長さが決まっていて、Trip Advisorは決まっていないこと。\n",
    "・長さを調整する部分を見つけ出したいところだが、126の部分ではなかった。まだ考える必要がある。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e7e9f53a7e16853fec103b312bead0c8fac62e8daa5be37d711e7d09c796e8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
